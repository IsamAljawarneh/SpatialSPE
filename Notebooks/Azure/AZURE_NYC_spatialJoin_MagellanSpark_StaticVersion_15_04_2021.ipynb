{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,harsha2010:magellan:1.0.5-s_2.11,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6,org.apache.spark:spark-streaming_2.11:2.2.0,org.apache.spark:spark-sql_2.11:2.2.0\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\",\n        \"spark.dynamicAllocation.enabled\": false\n    }\n}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,harsha2010:magellan:1.0.5-s_2.11,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6,org.apache.spark:spark-streaming_2.11:2.2.0,org.apache.spark:spark-sql_2.11:2.2.0', u'spark.dynamicAllocation.enabled': False, u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1618656137341_0007</td><td>spark</td><td>busy</td><td><a target=\"_blank\" href=\"http://hn1-sspark.se1x413plcyuhpboysvltzxiva.fx.internal.cloudapp.net:8088/proxy/application_1618656137341_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn2-sspark.se1x413plcyuhpboysvltzxiva.fx.internal.cloudapp.net:30060/node/containerlogs/container_1618656137341_0007_01_000001/livy\">Link</a></td><td></td></tr></table>"}, "metadata": {}}], "metadata": {"cell_status": {"execute_time": {"duration": 49.02001953125, "end_time": 1618660516523.24}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "/**\n * @Description: a spatial join based on Filter-refine approach for NYC taxicab data\n * @author: Isam Al Jawarneh\n * @date: 02/02/2019\n *last update: 14/04/2021\n */", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 2, "cell_type": "code", "source": "sc.version", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1618656137341_0008</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-sspark.se1x413plcyuhpboysvltzxiva.fx.internal.cloudapp.net:8088/proxy/application_1618656137341_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn2-sspark.se1x413plcyuhpboysvltzxiva.fx.internal.cloudapp.net:30060/node/containerlogs/container_1618656137341_0008_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\nres1: String = 2.2.0.2.6.3.84-1"}], "metadata": {"cell_status": {"execute_time": {"duration": 762.51806640625, "end_time": 1618660551435.808}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 3, "cell_type": "code", "source": "import util.control.Breaks._\nimport org.apache.spark.sql.streaming.StreamingQueryListener\nimport org.apache.spark.util.random.XORShiftRandom\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.sql.types._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SQLImplicits\nimport org.apache.spark.sql.functions.from_json\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.Dataset\nimport org.apache.spark.sql.ForeachWriter\nimport magellan._\nimport magellan.index.ZOrderCurve\nimport magellan.{Point, Polygon}\n\nimport org.apache.spark.sql.magellan.dsl.expressions._\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.streaming.OutputMode\nimport org.apache.spark.sql.types.{\n  DoubleType,\n  StringType,\n  StructField,\n  StructType\n}\nimport org.apache.spark.sql.streaming._\nimport org.apache.spark.sql.streaming.Trigger\nimport org.apache.spark.sql.execution.streaming.MemoryStream\nimport org.apache.spark.sql.functions.{collect_list, collect_set}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.log4j.{Level, Logger}\nimport scala.collection.mutable\nimport scala.concurrent.duration.Duration\nimport java.io.{BufferedWriter, FileWriter}\nimport org.apache.commons.io.FileUtils\nimport java.io.File\nimport scala.collection.mutable.ListBuffer\nimport java.time.Instant\nimport org.apache.spark.util.CollectionAccumulator\nimport org.apache.spark.sql.DataFrame", "outputs": [{"output_type": "stream", "name": "stdout", "text": "import org.apache.spark.sql.DataFrame"}], "metadata": {"cell_status": {"execute_time": {"duration": 9338.98388671875, "end_time": 1618660560790.983}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 4, "cell_type": "code", "source": "val schemaNYCshort = StructType(Array(\n    StructField(\"vendorId\", StringType, false),\n    StructField(\"Pickup_longitude\", DoubleType, false),\n    StructField(\"Pickup_latitude\", DoubleType, false),\n    StructField(\"Trip_distance\", DoubleType, false)))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "schemaNYCshort: org.apache.spark.sql.types.StructType = StructType(StructField(vendorId,StringType,false), StructField(Pickup_longitude,DoubleType,false), StructField(Pickup_latitude,DoubleType,false), StructField(Trip_distance,DoubleType,false))"}], "metadata": {"cell_status": {"execute_time": {"duration": 1281.094970703125, "end_time": 1618660562085.16}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 5, "cell_type": "code", "source": "// a user defined function to get geohash from long/lat point \nval geohashUDF = udf{(curve: Seq[ZOrderCurve]) => curve.map(_.toBase32())}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "geohashUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,ArrayType(StringType,true),Some(List(ArrayType(org.apache.spark.sql.types.ZOrderCurveUDT@1a72e6cd,true))))"}], "metadata": {"cell_status": {"execute_time": {"duration": 2287.19091796875, "end_time": 1618660564389.564}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 6, "cell_type": "code", "source": "//\"wasb[s]://<BlobStorageContainerName>@<StorageAccountName>.blob.core.windows.net/<path>\"\nval tripsWithGeoNeigh =spark.read.format(\"csv\").option(\"header\", \"true\").schema(schemaNYCshort).csv(\"wasbs://sspark-2021-04-17t10-30-16-344z@ssparkhdistorage.blob.core.windows.net/datasets/nyc.csv\")//.withColumn(\"point\", point($\"Pickup_longitude\",$\"Pickup_latitude\"))\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "tripsWithGeoNeigh: org.apache.spark.sql.DataFrame = [vendorId: string, Pickup_longitude: double ... 2 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 5339.97705078125, "end_time": 1618660569741.422}}, "collapsed": false}}, {"execution_count": 7, "cell_type": "code", "source": "tripsWithGeoNeigh.show(2)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------+------------------+------------------+-------------+\n|vendorId|  Pickup_longitude|   Pickup_latitude|Trip_distance|\n+--------+------------------+------------------+-------------+\n|       2|-73.95267486572266|40.723175048828125|         3.56|\n|       2|-73.97161102294922| 40.67610549926758|         3.79|\n+--------+------------------+------------------+-------------+\nonly showing top 2 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 9351.41796875, "end_time": 1618660579105.325}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 8, "cell_type": "code", "source": "val precision = 30", "outputs": [{"output_type": "stream", "name": "stdout", "text": "precision: Int = 30"}], "metadata": {"cell_status": {"execute_time": {"duration": 766.674072265625, "end_time": 1618660689689.291}}, "collapsed": false}}, {"execution_count": 9, "cell_type": "code", "source": "//getting plain data from CSV file and use UDF to get geohashes\nval trips =spark.read.format(\"csv\").option(\"header\", \"true\").schema(schemaNYCshort).csv(\"wasbs://sspark-2021-04-17t10-30-16-344z@ssparkhdistorage.blob.core.windows.net/datasets/nyc.csv\").withColumn(\"point\", point($\"Pickup_longitude\",$\"Pickup_latitude\"))\nval ridesGeohashed = trips.withColumn(\"index\", $\"point\" index  precision).withColumn(\"geohashArray1\", geohashUDF($\"index.curve\"))//.select($\"id\", $\"vendorId\", $\"point\",$\"geohashArray\",$\"Trip_distance\")\nval explodedRidesGeohashed = ridesGeohashed.explode(\"geohashArray1\", \"geohash\") { a: mutable.WrappedArray[String] => a }", "outputs": [{"output_type": "stream", "name": "stdout", "text": "warning: there was one deprecation warning; re-run with -deprecation for details\nexplodedRidesGeohashed: org.apache.spark.sql.DataFrame = [vendorId: string, Pickup_longitude: double ... 6 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 2284.548095703125, "end_time": 1618660693277.374}}, "collapsed": false}}, {"execution_count": 11, "cell_type": "code", "source": "explodedRidesGeohashed.show(2,false)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------+------------------+------------------+-------------+---------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\n|vendorId|Pickup_longitude  |Pickup_latitude   |Trip_distance|point                                        |index                                                                                                                                                 |geohashArray1|geohash|\n+--------+------------------+------------------+-------------+---------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\n|2       |-73.95267486572266|40.723175048828125|3.56         |Point(-73.95267486572266, 40.723175048828125)|[[ZOrderCurve(-73.9599609375, 40.7208251953125, -73.948974609375, 40.726318359375, 30, 7335093599358418944, 011001011100101101111100110001),Contains]]|[dr5rtj]     |dr5rtj |\n|2       |-73.97161102294922|40.67610549926758 |3.79         |Point(-73.97161102294922, 40.67610549926758) |[[ZOrderCurve(-73.98193359375, 40.67138671875, -73.970947265625, 40.6768798828125, 30, 7335089871326806016, 011001011100101101111001011000),Contains]]|[dr5rks]     |dr5rks |\n+--------+------------------+------------------+-------------+---------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\nonly showing top 2 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 5299.364990234375, "end_time": 1618660727902.158}}, "collapsed": false}}, {"execution_count": 12, "cell_type": "code", "source": "explodedRidesGeohashed.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res9: Long = 1441584"}], "metadata": {"cell_status": {"execute_time": {"duration": 11342.7109375, "end_time": 1618660869304.844}}, "collapsed": false}}, {"execution_count": 16, "cell_type": "code", "source": "//explode geohashes covering each neighborhood\nval rawNeighborhoodsNYC = spark.sqlContext.read.format(\"magellan\").option(\"type\", \"geojson\").load(\"wasbs://sspark-2021-04-17t10-30-16-344z@ssparkhdistorage.blob.core.windows.net/neighborhoods/\").select($\"polygon\", $\"metadata\"(\"neighborhood\").as(\"neighborhood\")).cache()\n\nval neighborhoodsNYC = rawNeighborhoodsNYC.withColumn(\"index\", $\"polygon\" index  30).select($\"polygon\", $\"index\", \n      $\"neighborhood\").cache()\n\nval zorderIndexedNeighborhoodsNYC = neighborhoodsNYC.withColumn(\"index\", explode($\"index\")).select(\"polygon\", \"index.curve\", \"index.relation\",\"neighborhood\")\nval geohashedNeighborhoodsNYC= neighborhoodsNYC.withColumn(\"geohashArray\", geohashUDF($\"index.curve\"))\n\nval explodedgeohashedNeighborhoodsNYC = geohashedNeighborhoodsNYC.explode(\"geohashArray\", \"geohash\") { a: mutable.WrappedArray[String] => a }", "outputs": [{"output_type": "stream", "name": "stdout", "text": "warning: there was one deprecation warning; re-run with -deprecation for details\nexplodedgeohashedNeighborhoodsNYC: org.apache.spark.sql.DataFrame = [polygon: polygon, index: array<struct<curve:zordercurve,relation:string>> ... 3 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 2281.367919921875, "end_time": 1618661046156.94}}, "collapsed": false}}, {"execution_count": 17, "cell_type": "code", "source": "explodedgeohashedNeighborhoodsNYC.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res20: Long = 3434"}], "metadata": {"cell_status": {"execute_time": {"duration": 761.48388671875, "end_time": 1618661047831.183}}, "collapsed": false}}, {"execution_count": 18, "cell_type": "code", "source": "//joining geohashed trips with exploded geohashed neighborhood using filter-and-refine approach (.where($\"point\" within $\"polygon\") is refine --> using the brute force method ray casting for edge cases or false positives)\nval rawTripsJoinedNYC = explodedRidesGeohashed.join(explodedgeohashedNeighborhoodsNYC, explodedRidesGeohashed(\"geohash\") === explodedgeohashedNeighborhoodsNYC(\"geohash\"))/*.select(\"point\", \"neighborhood\",\"id\")*/.where($\"point\" within $\"polygon\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "rawTripsJoinedNYC: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [vendorId: string, Pickup_longitude: double ... 11 more fields]"}], "metadata": {"cell_status": {"execute_time": {"duration": 773.453125, "end_time": 1618661049706.145}}, "collapsed": false}}, {"execution_count": 19, "cell_type": "code", "source": "rawTripsJoinedNYC.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res22: Long = 1441730"}], "metadata": {"cell_status": {"execute_time": {"duration": 5297.64013671875, "end_time": 1618661090133.825}}, "collapsed": false}}, {"execution_count": 23, "cell_type": "code", "source": "val Top_N = rawTripsJoinedNYC.groupBy('neighborhood).count().orderBy($\"count\".desc)\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Top_N: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [neighborhood: string, count: bigint]"}], "metadata": {"cell_status": {"execute_time": {"duration": 759.873046875, "end_time": 1618661214304.69}}, "collapsed": false}}, {"execution_count": 24, "cell_type": "code", "source": "Top_N.show(10)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------------------+------+\n|       neighborhood| count|\n+-------------------+------+\n|             Harlem|157463|\n|        East Harlem|140966|\n|       Williamsburg|117837|\n|            Astoria| 73483|\n|           Elmhurst| 58186|\n| Bedford-Stuyvesant| 56020|\n|   Long Island City| 45698|\n|         Park Slope| 43326|\n|Morningside Heights| 42351|\n|        Fort Greene| 42173|\n+-------------------+------+\nonly showing top 10 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 5286.97802734375, "end_time": 1618661220558.944}}, "collapsed": false}}, {"execution_count": 21, "cell_type": "code", "source": "//groupBy aggregation by neighborhood (or geohash at a granular level to check everything is OK!)\nrawTripsJoinedNYC.groupBy('neighborhood).count().orderBy($\"count\".desc).count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res26: Long = 223"}], "metadata": {"cell_status": {"execute_time": {"duration": 9361.426025390625, "end_time": 1618661150833.602}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}